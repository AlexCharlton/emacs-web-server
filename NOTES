                                                           -*- org -*-

* Notes
* Tasks [10/20]
** DONE handle large files
When large files arrive quickly, the filter functions are called while
they are still running on the previous chunk, this leads to nasty race
conditions for the state of the request object.

Either introduce some check to wait on new input if input is currently
being parsed, or wait until all input has arrived before doing any
parsing.

Now using an =active= field on request objects to avoid race
conditions when new header text is received while the parsing function
is still active.

** TODO robustness to bad requests [0/2]
Low priority, just [[*running%20behind%20a%20proxy][run behind a proxy]].

*** TODO request timeout
*** TODO maximum request size
** TODO authentication [1/2]
*** DONE Basic
http://en.wikipedia.org/wiki/Basic_access_authentication

*** TODO Digest
http://en.wikipedia.org/wiki/Digest_access_authentication

If this is implemented, it would be good to implement some safeguards
against common attacks.

#+begin_quote
- Server nonce is allowed to contain timestamps. Therefore the server
  may inspect nonce attributes submitted by clients, to prevent replay
  attacks.
- Server is also allowed to maintain a list of recently issued or used
  server nonce values to prevent reuse.
#+end_quote

** TODO tutorial or usage section in documentation [0/3]
*** TODO running behind an https proxy
*** TODO running behind a proxy
*** TODO documentation for running in a chroot jail
see https://wiki.archlinux.org/index.php/nginx#Installation_in_a_chroot

** incremental handler calls
not sure if the extra performance is worth the added complexity

Before the header is fully parsed, call any potential handlers.
Include a field in the request object to indicate that the request
isn't finished being received so handlers can return and wait to be
called again.

Also, put a catch in the filter function and allow the =headers=
function on the request object to throw to said catch aborting the
handler and waiting for the rest of the input.

** DONE Documentation [6/6]
- [X] introduction
- [X] handlers
- [X] request headers
- [X] usage examples
- [X] list of functions

Notes to touch upon
- [X] how to set content type

** DONE Handle POST requests
1. read standard for POST data
2. parse multi-line headers with boundaries

For now keep this all incremental and in ws-filter.

** DONE Makefile
- byte-compile
- package
- test
- benchmark
** DONE catch errors and return an error code
include an easy error handler like the 404 handler

** DONE better parsing of multipart form blocks
parse more than just the content-type headers.

** DONE non-multipart form data
e.g., parameter strings

** DONE some more convenience functionality [6/6]
- [X] strip and parse URL query string
- [X] parse urlencoded post data
- [X] think about defaulting to (name . content) for form elements
- [X] maybe don't require a non-nil return to cancel the connection,
      instead only keep open if :keep-open is returned
- [X] function to send a file (with mime handling)
- [X] send a 404 with some default text

** CANCELED Lazy header processing
   - State "CANCELED"   from "TODO"       [2013-12-25 Wed 12:21] \\
     premature optimization
Use lazy sequence functions for header a-list to avoid parsing all
headers.  For regexp matchers should stop when matched header is
encountered (often the first one when :GET), For function matchers
provide lazy version of assoc.

Also, there is the issue of how a lazy request for more parameters
should act before all incoming text has been received.  Emacs does not
provide a light-weight mechanism for a function to wait for incoming
process text without something gross like the =(sit-for 0.1)= used in
the test suite.

** TODO use gnutls for https
low priority -- just [[*running%20behind%20an%20https%20proxy][run behind an https proxy]].

This will be a pain, and will require expanding [[info:emacs-gnutls]] to
add support for starting server processes, currently only client
processes are supported.
